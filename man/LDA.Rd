% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_gibbs_LDA.R
\name{LDA}
\alias{LDA}
\title{Creates Latent Dirichlet Allocation model.}
\usage{
LDA(n_topics, vocabulary, doc_topic_prior = 1/n_topics,
  topic_word_prior = 1/n_topics, ...)
}
\arguments{
\item{n_topics}{desired number of topics. Also knows as \bold{K}.}

\item{vocabulary}{vocabulary in a form of \code{character} vector or class of
\code{text2vec_vocab}}

\item{doc_topic_prior}{prior for document-topic multinomial distribution.
Also knows as \bold{alpha}.}

\item{topic_word_prior}{prior for topic-word multinomial distribution.
Also knows as \bold{eta}.}

\item{...}{arguments passed to other methods (not used at the moment).}
}
\description{
\bold{Iterative algorithm}. Model can be fitted
via Collapsed Gibbs Sampling algorithm using \code{fit} or \code{fit_transf} methods.
}
\examples{
library(text2vec)
data("movie_review")
N = 100
tokens = movie_review$review[1:N] \%>\% tolower \%>\% word_tokenizer
it = itoken(tokens, ids = movie_review$id[1:N])
v = create_vocabulary(it) \%>\%
prune_vocabulary(term_count_min = 5, doc_proportion_max = 0.2)
dtm = create_dtm(it, vocab_vectorizer(v), 'lda_c')
     lda_model = LDA(n_topics = 10, vocabulary = v,
     doc_topic_prior = 0.1, topic_word_prior = 0.1,
     check_convergence_every_n = 5, verbose = T)
doc_topic_distr = fit_transf(lda_model, dtm, n_iter = 10)
}

